---
title: "Simple Linear Regression"
author: "<Denzel Wambua>"
date: "`r Sys.Date()`"
output:
  html_notebook: default
  html_document:
    toc: true
    toc_depth: 4
    number_sections: true
    fig_width: 6
    fig_height: 6
    self_contained: false
    keep_md: true
  pdf_document: 
    toc: true
    toc_depth: 4
    number_sections: true
    fig_height: 6
    fig_crop: false
    keep_tex: true
    latex_engine: xelatex
  word_document:
    toc: true
    toc_depth: 4
    number_sections: true
    fig_width: 6
    keep_md: true
---

# Load the Dataset

```{r}
if (!"pacman" %in% installed.packages()[, "Package"]) {
install.packages("pacman", dependencies = TRUE)
library("pacman", character.only = TRUE)
}
pacman::p_load("here")
knitr::opts_knit$set(root.dir = here::here())
```

```{r}
pacman::p_load("readr")
clv_data <- read_csv("./data/clv_data.csv")
head(clv_data)
```

# View the Data Types

```{r show_data_types_1, echo=TRUE, message=FALSE, warning=FALSE}
sapply(clv_data, class)


```

```{r show_data_types_2, echo=TRUE, message=FALSE, warning=FALSE}
str(clv_data)

```

```{r show_data_types_3, echo=TRUE, message=FALSE, warning=FALSE}
summary(clv_data)
```

# Variance:

```{r distribution_variance, echo=TRUE, message=FALSE, warning=FALSE}
#'sapply()' is designed to apply a function to a variable in a dataset
#In this case, I used 'sapply()' to apply the 'var()' function used to compute the variance.
#High variability means that the values are less consistent, thus making it harder to make predictions.
sapply(clv_data[,], var)

```

# Standard Deviation:

```{r distributiom_standard_deviation, echo=TRUE, message=FALSE, warning=FALSE}
sapply(clv_data[,],sd)

```

# Kurtosis

```{r distribution_kurtosis, echo=TRUE, message=FALSE, warning=FALSE}
#Informs how often outliers occur 
#Different formulas for calculating hence we specify type 2 which is used in other software
#Kurtosis = 3 -> medium no. of outliers
#Kurtosis<3 -> low no. of ouliers and vice versa
pacman::p_load("e1071")
sapply(clv_data[,],kurtosis, type=2)

```

# Skewness

```{r distribution_skewness, echo=TRUE, message=FALSE, warning=FALSE}
#Used to ID the asymmetry of distribution of results
#Similar to kurtosis we have type 2 which is widely used by other apps :)
#-0.4<Skewness<0.4 inclusive implies no skew i.e it is a normal distribution
#Above 0.4 implies +ve skew
#below -0.4 implies -ve skew: a left-skewed distribution
sapply(clv_data[,], skewness, type = 2)

```

# Covariance

```{r distribution_covariance, echo=TRUE, message=FALSE, warning=FALSE}
#Indicates the direction of the linear relationship betweeen 2 variables
#Assesses whether increase in one leads to an increase in the other
#+ve covariance -> when one increases the other increases
#-ve  covariance -> when one increases the other decreases
#Zero covariance -> no relationship
#Shows direction of relationship but not strength
cov(clv_data, method = "spearman")
```

# Correlation

```{r distribution_correlation_1, echo=TRUE, message=FALSE, warning=FALSE}
#Strong correlation enables better prediction of independent variable
#Only useful if there is linear association/strong correlation
#Spearman's rank correlation rho is used to measure statistical significance of the correlation
#Monotomic relationship -> one var increases and the other either increases consistently or consistently decreases
#Rate of change may vary but direction is preserved
cor.test(clv_data$customer_lifetime_value, clv_data$purchase_frequency, method = "spearman")
```

To view correlation of all variables

```{r distribution_correlation_2, echo=TRUE, message=FALSE, warning=FALSE}
cor(clv_data, method = "spearman")

```

# Basic Visualizations

```{r visualization_histogram, echo=TRUE, fig.width=6, message=FALSE, warning=FALSE}
# par(mfrow = c(1, 2)) This  is used to divide the area used to plot the visualization into a 1 row by 2 columns grid
# for (i in 1:2) This is used to identify the variable (column) that is being processed
# clv_data[[i]] This is used to extract the i-th column as a vector
# hist() This is the fnctn used to plot the histogram
par(mfrow = c(1, 2))
for (i in 1:2) {
  if (is.numeric(clv_data[[i]])){
    hist(clv_data[[i]],
         main = names(clv_data)[i],
         xlab = names(clv_data)[i])
  } else {
    message(paste("Column", names(clv_data)[i], "is not numeric and will be skipped"))
  }
}

```

```{r visualization_boxplot, echo=TRUE, fig.width=6, message=FALSE, warning=FALSE}
par(mfrow = c(1, 2))
for (i in 1:2) {
  if (is.numeric(clv_data[[i]])) {
    boxplot(clv_data[[i]], main = names(clv_data)[i])
  } else {
    message(paste("Column", names(clv_data)[i], "is not numeric and will be skipped"))
  }
}

```

```{r missing_data_plot, echo=TRUE, fig.width=6, error=FALSE, warning=FALSE}
pacman::p_load("Amelia")

missmap(clv_data, col = c("red", "grey"), legend = TRUE)

```

# Correlation Plot

```{r correlation_plot, echo=TRUE, fig.width=6, message=FALSE, warning=FALSE}
pacman::p_load("ggcorrplot")
ggcorrplot(cor(clv_data[,]))

```

# Scatter Plot

```{r scatter_plot_1, echo=TRUE, fig.width=6, message=FALSE, warning=FALSE}
pacman::p_load("corrplot")

pairs(clv_data$customer_lifetime_value ~ . , data = clv_data, col = clv_data$customer_lifetime_value)

```

```{r scatter_plot_2, echo=TRUE, fig.width=6, message=FALSE, warning=FALSE}
pacman::p_load("ggplot2")
ggplot(clv_data,
       aes(x = purchase_frequency, y = customer_lifetime_value)) + 
  geom_point() +
geom_smooth(method = lm) +
  labs(
    title = "Relationship between customer lifetime value and purchase frequency",
    x = "Purchase Frequency",
    y = "Customer Lifetime Value"
  )
```

# Statistical test of Linear Regression

```{r}
slr_test <- lm(customer_lifetime_value ~ purchase_frequency, data = clv_data)

#To view result
summary(slr_test)
```

```{r}
#Obtain confidence intervals
confint(slr_test, level = 0.95)
```

# Diagnostic EDA

Diagnostic EDA tests validity of the model's assumptions before interpreting results. This helps prevent incorrect conclusions

## Test of Linearity

```{r test_of_linearity, echo=TRUE, fig.width=6, message=FALSE, warning=FALSE}
plot(slr_test, which = 1)

# Tests whether relationship between dependent and independent variables is linear
# A plot of residuals vs fitted values enables test for linearity
# For the model to pass there should be no pattern in the distribution of residuals and the residuals should be randomly placed around the 0.0 residual line
# i.e the residuals should randomly vary around the mean of the value of the response variable
```

## Test of Independence of Errors

This test is necessary to confirm each observation is independent of each other.

It helps to identify autocorrelation which occurs when data is collected over a close period of time or when an observation is related to another.

Autocorrelation leads to underestimated standard errors and inflated t-statistics / findings appear bigger than they actually are.

Durbin Watson Test

-   H0 -\> There is no autocorrelation (null hypothesis)

-   H1 -\> There is autocorrelation

If the p-value \> 5, no evidence to reject null hypothesis "There is no autocorrelation"

```{r test_of_independence_of_errors, echo=TRUE, message=FALSE, warning=FALSE}
pacman::p_load("lmtest")
dwtest(slr_test)
#The results show a p-value of 0.1573 therefore the test of independence of errors around the regression line passes
```

## Test of Normality

It assesses whether the residuals are normally distributed i.e most residuals(errors) are close to zero and large errors are rare

A Q-Q plot can be used for this

It is a scatter-plot of the quantities of the residuals against quantiles of a normal distribution

Quantiles are statistical values that divide a data set or probability into equal-sized intervals e.g quartiles, percentiles, deciles(10 equal parts) etc

If the points in the plot fall along a straight line, then the normality assumption is satisfied.

```{r test_of_normality, echo=TRUE, fig.width=6, message=FALSE, warning=FALSE}
plot(slr_test, which = 2)

```

# Test of Homoscedasticity

Homoscedasticity requires that the spread of residuals should be constant across all levels of the independent variable. A scale-location plot (a.k.a. spread-location plot) can be used to conduct a test of homoscedasticity.

The x-axis shows the fitted (predicted) values from the model and the y-axis shows the square root of the standardized residuals. The red line is added to help visualize any patterns.

In a model with homoscedastic errors (equal variance across all predicted values):

• Points should be randomly scattered around a horizontal line

• The smooth line should be approximately horizontal

• The vertical spread of points should be roughly equal across all fitted values

• No obvious patterns, funnels, or trends should be visible

Points forming a cone shape that widens from left to right suggests heteroscedasticity with increasing variance for larger fitted values.

```{r test_of_homoscedasticity, echo=TRUE, fig.width=6, message=FALSE, warning=FALSE}
plot(slr_test, which = 3)

```

# Quantitative Validation of Assumptions

The graphical representations of the various tests of assumptions should be accompanied by quantitative values. The gvlma package(Global Validation of Linear Models Assumptions) is useful for this purpose.

```{r}
pacman::p_load("gvlma")
gvlma_results <- gvlma(slr_test)
summary(gvlma_results)
```
